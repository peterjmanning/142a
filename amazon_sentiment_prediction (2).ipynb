{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O891MFFH7gHi"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e11fkYU9LBqF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d61befe-b9b2-473e-9a52-a737b093190b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ASwf0Hhg7X1u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3760c2a-8f60-470d-cbf7-b36449925991"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%%capture` not found.\n"
          ]
        }
      ],
      "source": [
        "### misc stuffs\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "### torch stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "%%capture"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#must print /content, assuming your structure is content / drive/ myDrive/ 142a colab/ (test or train csv's)\n",
        "test = pd.read_csv('drive/MyDrive/142a colab/test.csv', usecols=[0,2], names = ['posRating', 'text'], nrows= 3000) #rows can be changed to increase time taken for preprocessing\n",
        "train = pd.read_csv('drive/MyDrive/142a colab/train.csv', usecols = [0, 2], names = ['posRating', 'text'], nrows = 12000)\n",
        "train.posRating -= 1\n",
        "test.posRating -= 1\n",
        "train, val = train_test_split(train, test_size = 0.1, random_state= 69)\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "Lq0kXHs1K1HK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU Model"
      ],
      "metadata": {
        "id": "vx3D3qN4x2BG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(df):\n",
        "    docs = nlp.pipe(df['text'].str.lower(), batch_size=50, n_process=-1)\n",
        "    return [[token.text for token in doc if not token.is_punct and not token.is_space and not token.is_stop] for doc in docs]\n",
        "\n",
        "def padTrunc(tokenized_docs, max_length= 30, padToken = '<PAD>'):\n",
        "    num_docs = len(tokenized_docs)\n",
        "    result = np.full((num_docs, max_length), padToken, dtype=object)  # Use `object` for string tokens\n",
        "\n",
        "    for i, doc in enumerate(tokenized_docs):\n",
        "\n",
        "        truncated = doc[:max_length]  # Truncate to max_length\n",
        "        result[i, :len(truncated)] = truncated  # Place truncated tokens into the array\n",
        "\n",
        "    return result\n",
        "\n",
        "def embed():\n",
        "    embeddings = {}\n",
        "    with open('drive/MyDrive/142a colab/glove.6B.50d.txt', \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype=\"float32\")\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "def tokenToEmbedding(tokenized_docs, glove_dict, embedding_dim):\n",
        "    \"\"\"\n",
        "    Convert tokenized documents into embedding vectors.\n",
        "\n",
        "    Args:\n",
        "    - tokenized_docs (list of lists): Tokenized and padded documents.\n",
        "    - glove_dict (dict): Pre-loaded GloVe dictionary mapping words to embeddings.\n",
        "    - embedding_dim (int): Dimension of GloVe embeddings.\n",
        "\n",
        "    Returns:\n",
        "    - np.ndarray: 3D array of embeddings (num_docs, max_length, embedding_dim).\n",
        "    \"\"\"\n",
        "    # Create a 3D array to hold embeddings\n",
        "    num_docs = len(tokenized_docs)\n",
        "    max_length = len(tokenized_docs[0])  # Assume all documents are padded to the same length\n",
        "    embeddings = np.zeros((num_docs, max_length, embedding_dim), dtype=np.float32)\n",
        "\n",
        "    for i, doc in enumerate(tokenized_docs):\n",
        "        for j, token in enumerate(doc):\n",
        "            if token in glove_dict:\n",
        "                embeddings[i, j] = glove_dict[token]\n",
        "            else:\n",
        "                embeddings[i, j] = np.zeros(embedding_dim)  # Use zero vector for OOV or <PAD>\n",
        "\n",
        "    return embeddings\n"
      ],
      "metadata": {
        "id": "2mPr3oOiLKZY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = embed()"
      ],
      "metadata": {
        "id": "sg-VzLmcY-e-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preProcess(df):\n",
        "    mat = padTrunc(tokenizer(df))\n",
        "    Emb = torch.tensor(tokenToEmbedding(mat, embeddings, 50))\n",
        "    Lbl = torch.tensor(df['posRating'].values).float()\n",
        "    return TensorDataset(Emb, Lbl)"
      ],
      "metadata": {
        "id": "fyGA1b2tY_IR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MAX_SEQ_LEN = 30      fixed sequence length as 30, close to mean length\n",
        "#EMBEDDING_DIM = 50    dimension of GloVe word embeddings\n",
        "\n",
        "# Initialize DataLoaders\n",
        "trainMatrix = preProcess(train)\n",
        "valMatrix = preProcess(val)\n",
        "train_loader = DataLoader(trainMatrix, batch_size = 27, shuffle = True)\n",
        "val_loader = DataLoader(valMatrix, batch_size = 20, shuffle = False)"
      ],
      "metadata": {
        "id": "ggFmrmzRY-5_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUSentiment(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, drop):\n",
        "        super(GRUSentiment, self).__init__()\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout = drop)\n",
        "        self.output_layer = nn.Linear(hidden_size, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        gru_out, h_n = self.gru(x)  # gru_out: (batch, sequence length (30) , hidden_size) H_n: (numLayers, batch_size, hidden_size)\n",
        "        output = self.output_layer(h_n[-1]).flatten() # last hidden state: (batch, hidden) --> h_n[-1] yields last hidden state, flatten converts shape from 45,1 to 45.\n",
        "        return output # (batch)\n",
        "\n",
        "def get_loss_fn():\n",
        "    return nn.BCEWithLogitsLoss()\n",
        "\n",
        "def get_validation_accuracy(model, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    totalPreds = 0\n",
        "    yPreds = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            x = batch[0].to(device)\n",
        "            y = batch[1].to(device)\n",
        "            probs = torch.sigmoid(model.forward(x))\n",
        "            preds = (probs >= 0.5).float()\n",
        "            yPreds.extend(preds.cpu().numpy())\n",
        "            correct += (preds == y).sum().item()\n",
        "            totalPreds += preds.shape[0]\n",
        "    return (correct / totalPreds), f1_score(val['posRating'].values, yPreds)\n",
        "\n",
        "def train_model(model,device):\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.00015)\n",
        "    criterion = get_loss_fn()\n",
        "\n",
        "    for epoch in range(20):\n",
        "        model.train()\n",
        "\n",
        "        for batch in train_loader:\n",
        "            x = batch[0].to(device)\n",
        "            y = batch[1].to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            logits = model.forward(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        val_accuracy = get_validation_accuracy(model,device)\n",
        "        print(f\"Epoch {epoch} Validation Accuracy = {val_accuracy[0]:.4f}\")\n",
        "        print(f\"F1 Score = {val_accuracy[1]:.4f}\")\n",
        "\n",
        "    return\n",
        "\n"
      ],
      "metadata": {
        "id": "Sp1J2bG8bP95"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Model hyperparameters & Training\n",
        "\"\"\"\n",
        "input_size = 50\n",
        "hidden_size = 150 # Increased hidden size for better representation\n",
        "num_layers = 2  # Reduced number of layers to prevent overfitting\n",
        "drop = 0.1 #model parameter dropout probability\n",
        "model = GRUSentiment(input_size, hidden_size, num_layers, drop)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_model(model, device)"
      ],
      "metadata": {
        "id": "OgIizjnLjTMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(preProcess(test), batch_size = 20, shuffle = False)"
      ],
      "metadata": {
        "id": "C69-Krgwb2wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, device, test_loader):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            x = batch[0].to(device)\n",
        "            logits = model(x)  # Raw logits\n",
        "            probs = torch.sigmoid(logits)  # Convert logits to probabilities\n",
        "            preds = (probs >= 0.5).float()  # Binary predictions\n",
        "            all_predictions.extend(preds.cpu().numpy())\n",
        "\n",
        "    return all_predictions"
      ],
      "metadata": {
        "id": "RWfXyM49cDsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = predict(model, device, test_loader)\n",
        "y_true = test.posRating\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "roc_auc = roc_auc_score(y_true, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")\n",
        "print(f\"Test F1-Score: {f1:.4f}\")\n",
        "print(f\"Test ROC-AUC: {roc_auc:.4f}\")"
      ],
      "metadata": {
        "id": "7msTrBX9cjne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpOaEfH-Voye"
      },
      "source": [
        "# Multinomial Naive Bayes Model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv('drive/MyDrive/142a colab/train.csv', header=None)\n",
        "test = pd.read_csv('drive/MyDrive/142a colab/test.csv', header=None)\n",
        "train.columns = ['posRating', 'first_review', 'second_review']\n",
        "test.columns = ['posRating', 'first_review', 'second_review']"
      ],
      "metadata": {
        "id": "aLMT2Sn2grBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Melt the train DataFrame\n",
        "train_long = pd.melt(\n",
        "    train,\n",
        "    id_vars='posRating',\n",
        "    value_vars=['first_review', 'second_review'],\n",
        "    var_name='original_column',\n",
        "    value_name='text'\n",
        ")\n",
        "train_long.drop(columns='original_column', inplace=True)\n",
        "\n",
        "# Melt the test DataFrame\n",
        "test_long = pd.melt(\n",
        "    test,\n",
        "    id_vars='posRating',\n",
        "    value_vars=['first_review', 'second_review'],\n",
        "    var_name='original_column',\n",
        "    value_name='text'\n",
        ")\n",
        "test_long.drop(columns='original_column', inplace=True)\n",
        "\n",
        "# Map the posRating values: 1 -> 0 (negative), 2 -> 1 (positive)\n",
        "rating_map = {1: 0, 2: 1}\n",
        "train_long['posRating'] = train_long['posRating'].replace(rating_map)\n",
        "test_long['posRating'] = test_long['posRating'].replace(rating_map)\n",
        "train_long.dropna(subset=['posRating', 'text'], inplace=True)\n",
        "test_long.dropna(subset=['posRating', 'text'], inplace=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZcEFBHFUg5s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWnhGZDqYQem"
      },
      "outputs": [],
      "source": [
        "#Sam\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Extract training and test data\n",
        "train_texts = train_long['text']\n",
        "train_labels = train_long['posRating']\n",
        "\n",
        "test_texts = test_long['text']\n",
        "test_labels = test_long['posRating']\n",
        "\n",
        "# Vectorize text (bag-of-words representation)\n",
        "vectorizer = TfidfVectorizer(stop_words='english')  # limit vocabulary for efficiency\n",
        "X_train = vectorizer.fit_transform(train_texts)\n",
        "X_test = vectorizer.transform(test_texts)\n",
        "\n",
        "# Initialize and train the Multinomial Naive Bayes model\n",
        "mnb_model = MultinomialNB()\n",
        "mnb_model.fit(X_train, train_labels)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = mnb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(test_labels, y_pred))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(test_labels, y_pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcb9jjbL962l"
      },
      "source": [
        "# Logistic Regression Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-Fh1lJI_sbl"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import os\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(os.getcwd())\n",
        "\n",
        "train = pd.read_csv('drive/MyDrive/142a colab/train.csv', header=None)\n",
        "test = pd.read_csv('drive/MyDrive/142a colab/test.csv', header=None)\n",
        "\n",
        "train.columns = ['posRating', 'subject', 'text']\n",
        "test.columns = ['posRating', 'subject', 'text']\n",
        "\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSiGJdKXOA0i"
      },
      "outputs": [],
      "source": [
        "x_train = train['text']\n",
        "y_train = train['posRating']\n",
        "\n",
        "x_test = test['text']\n",
        "y_test = test['posRating']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4e9PiOhNlB1"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loPy8CQ3OjDO"
      },
      "outputs": [],
      "source": [
        "X_train_tfidf = vectorizer.fit_transform(x_train)\n",
        "X_test_tfidf = vectorizer.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLTkVcpFOlgd"
      },
      "outputs": [],
      "source": [
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToXvCD7uO0kL"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr0gDCCBQlqQ"
      },
      "source": [
        "FastText Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ra1ISLtqQpYc"
      },
      "outputs": [],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0-39gLqQsru"
      },
      "outputs": [],
      "source": [
        "from gensim.models import FastText\n",
        "import os\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7Ph1XLGRxok"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('drive/MyDrive/142a colab/train.csv', header=None)\n",
        "test = pd.read_csv('drive/MyDrive/142a colab/test.csv', header=None)\n",
        "\n",
        "train.columns = ['posRating', 'subject', 'text']\n",
        "test.columns = ['posRating', 'subject', 'text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gPakXkZR27Y"
      },
      "outputs": [],
      "source": [
        "X_train = train['text'].astype(str).apply(lambda x: x.lower().split())\n",
        "y_train = train['posRating']\n",
        "\n",
        "X_test = test['text'].astype(str).apply(lambda x: x.lower().split())\n",
        "y_test = test['posRating']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgZEEAXQR5lj"
      },
      "outputs": [],
      "source": [
        "fasttext_model = FastText(sentences=X_train, vector_size=100, window=5, min_count=5, workers=4)\n",
        "\n",
        "def get_average_embedding(tokens, model):\n",
        "    valid_tokens = [t for t in tokens if t in model.wv]\n",
        "    if not valid_tokens:\n",
        "        return np.zeros(model.wv.vector_size)\n",
        "    return np.mean([model.wv[t] for t in valid_tokens], axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFmrqeJPR9iD"
      },
      "outputs": [],
      "source": [
        "X_train_emb = np.vstack(X_train.apply(lambda x: get_average_embedding(x, fasttext_model)))\n",
        "X_test_emb = np.vstack(X_test.apply(lambda x: get_average_embedding(x, fasttext_model)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eg7Dn7YJSA8V"
      },
      "outputs": [],
      "source": [
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_emb, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MAAPYZPSEDw"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test_emb)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy (FastText + LR):\", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7dHlT2_iJsf"
      },
      "source": [
        "# Conv1D CNN Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jMdFL9RmrMo"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "punc = string.punctuation\n",
        "\n",
        "# 读取数据\n",
        "test = pd.read_csv('drive/MyDrive/142a colab/test.csv', names=['posRating', 'subject', 'text'])\n",
        "train = pd.read_csv('drive/MyDrive/142a colab/train.csv', names=['posRating', 'subject', 'text'])\n",
        "\n",
        "# 数据清洗\n",
        "translation_table = str.maketrans(\"\", \"\", punc)\n",
        "for df in [train, test]:\n",
        "    df['subject'] = df['subject'].astype(str).str.translate(translation_table).str.lower().apply(word_tokenize)\n",
        "    df['text'] = df['text'].astype(str).str.translate(translation_table).str.lower().apply(word_tokenize)\n",
        "\n",
        "train.posRating -= 1\n",
        "test.posRating -= 1\n",
        "\n",
        "# 划分训练集和验证集\n",
        "train, val = train_test_split(train, test_size=0.005)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2q9QE0ofmyZe"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 合并subject和text列\n",
        "train['combined'] = train['subject'] + train['text']\n",
        "val['combined'] = val['subject'] + val['text']\n",
        "test['combined'] = test['subject'] + test['text']\n",
        "\n",
        "# 创建分词器\n",
        "tokenizer = Tokenizer(num_words=10000)  # 设置最大词汇数量\n",
        "tokenizer.fit_on_texts(train['combined'])\n",
        "\n",
        "# 将文本转化为序列\n",
        "X_train = tokenizer.texts_to_sequences(train['combined'])\n",
        "X_val = tokenizer.texts_to_sequences(val['combined'])\n",
        "X_test = tokenizer.texts_to_sequences(test['combined'])\n",
        "\n",
        "# 填充序列到固定长度\n",
        "maxlen = 100  # 设置最大序列长度\n",
        "X_train = pad_sequences(X_train, maxlen=maxlen, padding='post')\n",
        "X_val = pad_sequences(X_val, maxlen=maxlen, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=maxlen, padding='post')\n",
        "\n",
        "# 标签\n",
        "y_train = np.array(train['posRating'])\n",
        "y_val = np.array(val['posRating'])\n",
        "y_test = np.array(test['posRating'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGiVbuSjJwhP"
      },
      "outputs": [],
      "source": [
        "# 从 Google Drive 加载 GloVe 文件\n",
        "glove_path = 'drive/MyDrive/142a colab/glove.6B.50d.txt'\n",
        "\n",
        "# 加载 GloVe 词向量\n",
        "embeddings_index = {}\n",
        "with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = vector\n",
        "\n",
        "print(f\"Loaded {len(embeddings_index)} word vectors.\")\n",
        "\n",
        "# 获取 GloVe 词汇表的大小和嵌入维度\n",
        "embedding_dim = 50  # 使用 50 维的 GloVe 词向量\n",
        "\n",
        "# 创建一个嵌入矩阵\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
        "\n",
        "# 为每个词填充嵌入矩阵\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in embeddings_index:\n",
        "        embedding_matrix[i] = embeddings_index[word]\n",
        "    else:\n",
        "        embedding_matrix[i] = np.random.randn(embedding_dim)\n",
        "\n",
        "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "\n",
        "\n",
        "# 构建模型，使用预训练的 GloVe 嵌入\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
        "              output_dim=embedding_dim,\n",
        "              weights=[embedding_matrix],  # 使用 GloVe 词向量初始化嵌入层\n",
        "              input_length=maxlen,\n",
        "              trainable=False),  # 如果想要冻结词向量，设置为 False，或者 True 如果你希望训练嵌入\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')  # 输出层，用于二分类\n",
        "])\n",
        "\n",
        "# 编译模型\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 查看模型概述\n",
        "model.summary()\n",
        "\n",
        "# 训练模型\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=512,\n",
        "    validation_data=(X_val, y_val)\n",
        ")\n",
        "\n",
        "# 在测试集上评估模型\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc:.2f}\")\n",
        "\n",
        "# 保存模型\n",
        "model.save('sentiment_model.h5')\n",
        "\n",
        "# 加载模型\n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('sentiment_model.h5')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}